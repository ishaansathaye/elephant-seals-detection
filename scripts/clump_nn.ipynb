{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to download clumps_count and the folder of the images of the clumps off of the google drive! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in labels \n",
    "labels = pd.read_csv('clumps_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.dropna(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clump</th>\n",
       "      <th>Number of Adult Female Seals</th>\n",
       "      <th>Number of Baby Seals</th>\n",
       "      <th>Number of Adult Male Seals</th>\n",
       "      <th>Total Number of Seals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LS21323_DJI_0001_clump_0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LS21323_DJI_0001_clump_0002</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LS21323_DJI_0001_clump_0003</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LS21323_DJI_0001_clump_0004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LS21323_DJI_0001_clump_0005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         clump  Number of Adult Female Seals  \\\n",
       "0  LS21323_DJI_0001_clump_0001                           1.0   \n",
       "1  LS21323_DJI_0001_clump_0002                           4.0   \n",
       "2  LS21323_DJI_0001_clump_0003                           4.0   \n",
       "3  LS21323_DJI_0001_clump_0004                           1.0   \n",
       "4  LS21323_DJI_0001_clump_0005                           1.0   \n",
       "\n",
       "   Number of Baby Seals  Number of Adult Male Seals  Total Number of Seals  \n",
       "0                   1.0                         0.0                    2.0  \n",
       "1                   4.0                         0.0                    8.0  \n",
       "2                   5.0                         0.0                    9.0  \n",
       "3                   1.0                         0.0                    2.0  \n",
       "4                   1.0                         0.0                    2.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split \n",
    "train_labels, test_labels = train_test_split(labels, test_size=0.2, random_state=452)\n",
    "train_labels, val_labels = train_test_split(labels, test_size=1/8, random_state=452)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = train_labels['clump'].values\n",
    "val_imgs = val_labels['clump'].values \n",
    "test_imgs = test_labels['clump'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "class SealDataset(Dataset): \n",
    "    def __init__(self, img_dir, img_ids, labels, transform_pipeline):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_ids = img_ids \n",
    "        self.labels = labels \n",
    "        self.transform_pipeline = transform_pipeline\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels.iloc[idx]\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, f'{self.img_ids[idx]}.tif')\n",
    "        image = self.transform_pipeline(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        label = torch.tensor([\n",
    "            row['Number of Adult Male Seals'], \n",
    "            row['Number of Adult Female Seals'],\n",
    "            row['Number of Baby Seals']\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation pipeline \n",
    "transform_pl = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading to class\n",
    "img_dir = '../../elephant-seals-image-extraction/scripts/clumps' # img dir of clump imgs  \n",
    "\n",
    "train_set = SealDataset(img_dir, train_imgs, train_labels, transform_pl)\n",
    "val_set = SealDataset(img_dir, val_imgs, val_labels, transform_pl)\n",
    "test_set = SealDataset(img_dir, test_imgs, test_labels, transform_pl)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32)\n",
    "val_loader = DataLoader(val_set, batch_size=32)\n",
    "test_loader = DataLoader(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "class SealCountRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SealCountRegressor, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.regressor(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Brandon Kim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Brandon Kim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = SealCountRegressor()\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.PoissonNLLLoss(log_input=False) # poisson cuz count data \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.7343\n",
      "Val Loss: 0.5677\n",
      "Epoch 2 - Loss: 0.5502\n",
      "Val Loss: 0.4762\n",
      "Epoch 3 - Loss: 0.5081\n",
      "Val Loss: 0.4487\n",
      "Epoch 4 - Loss: 0.4912\n",
      "Val Loss: 0.4568\n",
      "Epoch 5 - Loss: 0.4788\n",
      "Val Loss: 0.4488\n",
      "Epoch 6 - Loss: 0.4698\n",
      "Val Loss: 0.4480\n",
      "Epoch 7 - Loss: 0.4646\n",
      "Val Loss: 0.4574\n",
      "Epoch 8 - Loss: 0.4610\n",
      "Val Loss: 0.4355\n",
      "Epoch 9 - Loss: 0.4585\n",
      "Val Loss: 0.4489\n",
      "Epoch 10 - Loss: 0.4605\n",
      "Val Loss: 0.4554\n"
     ]
    }
   ],
   "source": [
    "# model training \n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()*images.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {running_loss / len(train_loader.dataset):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        print(f'Val Loss: {val_loss / len(val_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE - Adult Males:   0.00\n",
      "MAE - Adult Females: 0.22\n",
      "MAE - Cubs:          0.45\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "model.eval()\n",
    "predicted_males = []\n",
    "predicted_females = []\n",
    "predicted_cubs = []\n",
    "\n",
    "actual_males = []\n",
    "actual_females = []\n",
    "actual_cubs = [] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        outputs = model(images).cpu()\n",
    "\n",
    "        for output, label in zip(outputs, labels): \n",
    "\n",
    "            predicted_males.append(output[0].item())\n",
    "            predicted_females.append(output[1].item())\n",
    "            predicted_cubs.append(output[2].item())\n",
    "\n",
    "            actual_males.append(label[0].item())\n",
    "            actual_females.append(label[1].item())\n",
    "            actual_cubs.append(label[2].item())\n",
    "\n",
    "predicted_males = np.array(predicted_males)\n",
    "predicted_females = np.array(predicted_females)\n",
    "predicted_cubs = np.array(predicted_cubs)\n",
    "actual_males = np.array(actual_males)\n",
    "actual_females = np.array(actual_females)\n",
    "actual_cubs = np.array(actual_cubs)\n",
    "\n",
    "mae_male = np.mean(np.abs(predicted_males - actual_males))\n",
    "mae_female = np.mean(np.abs(predicted_females - actual_females))\n",
    "mae_cubs = np.mean(np.abs(predicted_cubs - actual_cubs))\n",
    "\n",
    "print(f\"MAE - Adult Males:   {mae_male:.2f}\")\n",
    "print(f\"MAE - Adult Females: {mae_female:.2f}\")\n",
    "print(f\"MAE - Cubs:          {mae_cubs:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
